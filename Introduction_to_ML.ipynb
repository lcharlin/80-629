{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80629A Fall 2019\n",
    "=======\n",
    "\n",
    "Tutorial: An Introduction to Practical Machine Learning\n",
    "=============\n",
    "\n",
    "\n",
    "This tutorial provides a short introduction to the practice of machine learning. \n",
    "\n",
    "We assume that the user already has an understanding of the basic concepts that underlie the field. We review both methodological concepts like supervised learning and also use software libraries such as `scikit-learn`, `pandas`, and `numpy`. \n",
    "\n",
    "In particular, we will:\n",
    "1. load some data, \n",
    "2. fit different supervised models on variations of the data,\n",
    "3. and compare results.\n",
    "\n",
    "This tutorial is not meant to be exhaustive (references are provided throughout, and links to extra material are provided at the end).  \n",
    "\n",
    "### Authors: \n",
    "- Laurent Charlin <lcharlin@gmail.com>\n",
    "\n",
    "### Table of Content\n",
    "\n",
    "- [Section 0. Introduction](#introduction)\n",
    "- [Section 1. Data Pre-Processing](#pre-processing)\n",
    "- [Section 2. Modelling](#modelling)\n",
    "- [Section 3. Concluding Remarks](#concluding-remarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "### Section 0. Introduction\n",
    "We will use the example of a recommender system, i.e., a system which must recommend movies of interests to its users (e.g., Netflix). We will model user-movie preferences from a popular publicly available dataset (Movielens 1M). We will learn, from past user-movie ratings, to predict (missing/future) user-movie ratings from user socio-demographics and movie-tags data.\n",
    "\n",
    "Mathematically, we are interested in learning the (parameters of the) following function: \n",
    "\n",
    "$$ r_{um} = f_\\theta(x_u, x_m)$$ \n",
    "where \n",
    "- $u$ indexes users \n",
    "- $m$ indexes items\n",
    "- $r_{um}$ is u's rating for m (that user's preference) -- the dependent variable\n",
    "- $f_\\theta$ is some model parametrized by $\\theta$. For example, a linear regression with coefficients $\\theta$\n",
    "- $x_u$ are user u's covariates (e.g., age and occupation of this user)\n",
    "- $x_m$ are movie m's covariates (e.g., tags associated with the movie)\n",
    "\n",
    "The function $f$ can take several forms (in other words, we can use a variety of models for this task). In today's tutorial we will assume that the problem is a regression one and we will experiment with several models ranging from a simple linear regression model to a more complicated two-hidden layer neural network.\n",
    "\n",
    "\n",
    "### Machine Learning terminology\n",
    "\n",
    "It can be useful to think of machine learning as comprising three elements:\n",
    "1. Task (T)\n",
    "2. Experience (E)\n",
    "3. Performance measure (P).\n",
    "\n",
    "(a good description of these concepts is provided in [Ch. 5 of the Deep Learning Book](https://www.deeplearningbook.org/contents/ml.html))\n",
    "\n",
    "The intuition is that the task (T) is \"the type of problem you are trying the solve\" (e.g., classification, regression, anomaly detection), the experience (E) is \"how your data comes about\" (e.g., does it come with labels or not, do you observe it all at once or as a stream), and the performance (P) is \"how well your model does\". Standard performance measures include accuracy and mean-squared error.  \n",
    "\n",
    "Note that the above terminology does not define the model used to learn (fit) the data nor does it define the fitting procedure (e.g., gradient descent).\n",
    "\n",
    "Relationship to the problem of rating prediction:\n",
    "- Task: Our task is to predict user-movie ratings. It can be modelled in different ways (more on this during week 11), but here we will model it as a regression problem. \n",
    "- Experience: The experience is a supervised learning one because we are predicting some dependent variable (rating) from a set of independent variables\n",
    "- Performance measure: We will be using the mean-squared error.\n",
    "\n",
    "### Setting up the data \n",
    "\n",
    "For supervised learning, it is customary, to construct two data matrix $X$ and $Y$. The former, $X$, contains the covariates (features). It is a matrix of size $n \\times p$ with $n$ the number of examples and $p$ the dimensionality of each example (in other words the number of covariates associated with each example).  They are the input to the function. \n",
    "\n",
    "$$X = \\begin{bmatrix} \n",
    "x_{11} & x_{12} & \\ldots & x_{1p} \\\\\n",
    "\\vdots & \\vdots       &  \\ddots      & \\vdots \\\\ \n",
    "x_{n1} & x_{12} & \\ldots & x_{np} \\\\\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "The latter, $Y$, is a (column) vector of length $n$ which contains the labels (here ratings). $Y_1$ corresponds to the rating of $X_1$ (row  contains the labels (here ratings).\n",
    "\n",
    "$$\n",
    "Y = \\begin{bmatrix} \n",
    "r_1 \\\\\n",
    "r_2 \\\\\n",
    "\\vdots \\\\ \n",
    "r_n\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Of course, in a real problem we will differentiate the `train` and `test` sets, e.g., with $X_\\text{train}$ and $X_\\text{test}$. Same for the labels using, e.g., $Y_\\text{train}$ and $Y_\\text{test}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start\n",
    "\n",
    "Following this brief introduction, we now dive into the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '80-629'...\n",
      "remote: Enumerating objects: 18, done.\u001b[K\n",
      "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
      "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
      "remote: Total 70 (delta 11), reused 13 (delta 6), pack-reused 52\u001b[K\n",
      "\u001b[KUnpacking objects: 100% (70/70), done.\n"
     ]
    }
   ],
   "source": [
    "# We first download the repo to get access to data and some utility code (This is specifically for colab.)\n",
    "!rm -rf 80-629/\n",
    "!git clone https://github.com/lcharlin/80-629/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing the packages that we will need: \n",
    "- `reduce` function will come in handy to iteratively process data\n",
    "- `os` standard packages for performing system operations (e.g., opening files)\n",
    "- `re` package for regex\n",
    "- `sys` package to deal with system-level operations (here used to change the search path) \n",
    "- `time` package we will use to measure the duration of certain operations\n",
    "\n",
    "\n",
    "\n",
    "- `matplotlib` for plotting\n",
    "- `numpy` for linear-algebra computations\n",
    "- `pandas` for data wrangling \n",
    "- `sklearn` (scikit-learn) for machine learning models and useful machine learning related routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import os\n",
    "import re \n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import neural_network\n",
    "\n",
    "import sys\n",
    "sys.path += ['80-629/']\n",
    "from local_utils import DrawNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pre-processing'></a>\n",
    "# Section 1: Data Pre-Processing\n",
    "\n",
    "In the following we load data from several csv files and pre-process it. \n",
    "\n",
    "While this is not really machine learning, machine learning needs data and so knowing how to manipulate (and how to plot) data in python is quite useful. (In fact, in lots of use cases, data acquisition and cleaning will often take more of your time than running the machine learning models.)\n",
    "\n",
    "#### Details\n",
    "\n",
    "We will use the publically available [movielens dataset](https://grouplens.org/datasets/movielens/). The group behind movielens has released several useful datasets in the last 20 years. Here we will focus on the [ML-1M data](https://grouplens.org/datasets/movielens/1m/) (it contains 1M ratings) but we will also use movie tags from the [ML-20M dataset](https://grouplens.org/datasets/movielens/20m/) (20M ratings). \n",
    "\n",
    "Except for downloading the dataset (to save you some time), I have not processed nor modified the data in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR='80-629/'\n",
    "DATA_DIR=os.path.join(ROOT_DIR, 'dat/ml-1m/') # this is where most of our data lives\n",
    "DATA_DIR_20ML=os.path.join(ROOT_DIR, 'dat/ml-20m/') # for the tags data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Movie Names/Genres\n",
    "\n",
    "We begin by loading the data that describes movies in the ML-1M dataset. Each line in the file contains one entry in the following format `MovieID::Name::Genres`. \n",
    "\n",
    "After loading into a pandas `dataFrame` structure, we will have movie names (`mName`), IDs (`mid`), and movie genres (`mGenres`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_pd = pd.read_csv(os.path.join(DATA_DIR, 'movies.dat'), \n",
    "                        sep='::', \n",
    "                        names=['mid', 'mName', 'mGenres'], engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 3883 movies\n"
     ]
    }
   ],
   "source": [
    "print(f'The dataset contains {movies_pd.shape[0]} movies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid</th>\n",
       "      <th>mName</th>\n",
       "      <th>mGenres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mid                               mName                       mGenres\n",
       "0    1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1    2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2    3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3    4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4    5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(movies_pd.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas we can also search for movies by `mid` or by their name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid</th>\n",
       "      <th>mName</th>\n",
       "      <th>mGenres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "      <td>Action|Adventure|Thriller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mid             mName                    mGenres\n",
       "9   10  GoldenEye (1995)  Action|Adventure|Thriller"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid</th>\n",
       "      <th>mName</th>\n",
       "      <th>mGenres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>1433</td>\n",
       "      <td>Machine, The (1994)</td>\n",
       "      <td>Comedy|Horror</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mid                mName        mGenres\n",
       "1409  1433  Machine, The (1994)  Comedy|Horror"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mid = 10 \n",
    "display(movies_pd[movies_pd.mid==mid])\n",
    "\n",
    "name = 'Machine'\n",
    "display(movies_pd[movies_pd.mName.str.contains(name, \n",
    "                                               regex=False, case=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Ratings\n",
    "\n",
    "Using a similar routine as above, we load the ratings data which is in this format `UserID::MovieID::Rating::Timestamp`, and we will name the column UserID with `uid`, the column MovieID with `mid`, the rating with `rating`, and the time of the rating with `timestamp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>mid</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid   mid  rating  timestamp\n",
       "0    1  1193       5  978300760\n",
       "1    1   661       3  978302109\n",
       "2    1   914       3  978301968\n",
       "3    1  3408       4  978300275\n",
       "4    1  2355       5  978824291"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratings_pd = pd.read_csv(os.path.join(DATA_DIR, 'ratings.dat'), \n",
    "                         sep='::',\n",
    "                         names=['uid', 'mid', 'rating', 'timestamp'],\n",
    "                         parse_dates=['timestamp'],\n",
    "                         infer_datetime_format=True,\n",
    "                         engine='python')\n",
    "\n",
    "display(ratings_pd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 1000209 ratings, \n",
      "      from 6040 users, \n",
      "      and 3706 items.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"The dataset contains {ratings_pd.shape[0]} ratings, \n",
    "      from {ratings_pd.uid.nunique()} users, \n",
    "      and {ratings_pd.mid.nunique()} items.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load User Socio-Demographics Information\n",
    "\n",
    "The file is in this format `UserID::Gender::Age::Occupation::Zip-code`, which we will load in a `dataFrame` with the following column names `uid,gender,age,occupation,zip`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid gender  age  occupation    zip\n",
       "0    1      F    1          10  48067\n",
       "1    2      M   56          16  70072\n",
       "2    3      M   25          15  55117\n",
       "3    4      M   45           7  02460\n",
       "4    5      M   25          20  55455"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "users_pd = pd.read_csv(os.path.join(DATA_DIR, 'users.dat'),\n",
    "                       sep='::',\n",
    "                       names=['uid', 'gender', 'age', 'occupation', 'zip'],\n",
    "                       engine=\"python\")\n",
    "\n",
    "display(users_pd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This table contains 6040 users\n"
     ]
    }
   ],
   "source": [
    "print(f'This table contains {users_pd.shape[0]} users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further we will truncate the 5-digit zip codes and only keep the leading two digits. The reason is that we will treat this variable as a categorical one and with only ~6K users and >3.4K unique zip codes, it is unlikely that we can learn precise enough coefficients for this feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We originally have 100 different zip codes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    48\n",
       "1    70\n",
       "2    55\n",
       "3    02\n",
       "4    55\n",
       "Name: zip, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By only keep the first two digits of each zip code, we reduced the unique number of zip codes to 100.\n"
     ]
    }
   ],
   "source": [
    "print(f'We originally have {users_pd.zip.nunique()} different zip codes')\n",
    "users_pd['zip'] = users_pd['zip'].apply(lambda x: x[:2])\n",
    "display(users_pd['zip'].head())\n",
    "print(f'By only keep the first two digits of each zip code, \\\n",
    "we reduced the unique number of zip codes to {users_pd.zip.nunique()}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Movie Tags\n",
    "\n",
    "The remaining data to be loaded are the movie tags (we will actually use the tags from the ml-20M dataset). The tags are user generated. Further each movie-tag pair comes with an affinity score (intuitively, if numerous users have used a tag on a particular movie than the tag-movie pair will have a high affinity).\n",
    "\n",
    "We will load the csv data `movieId,tagId,relevance` into a `dataFrame` with the columns `mid,tid,relevance`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ml-20m tags\n",
    "tags_scores = pd.read_csv(os.path.join(DATA_DIR_20ML, 'genome-scores.csv.gz'), \n",
    "                          skiprows=1, \n",
    "                          names=['mid', 'tid', 'relevance'])\n",
    "display(tags_scores.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'The data contains {tags_scores.tid.nunique()} unique tags.')\n",
    "print(f'Affinities (relevances) are contained in the {tags_scores.relevance.min()}--{tags_scores.relevance.max()} range.')\n",
    "display(tags_scores.relevance.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we see that affinities (relevance) basically span the 0 to 1 range, and have an average of 0.12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also load the tag names. This will be useful for exploration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_names = pd.read_csv(os.path.join(DATA_DIR_20ML, 'genome-tags.csv'), skiprows=1, names=['tid', 'tName'])\n",
    "display(tags_names.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we loaded tag scores from a different dataset (ml-20M), we only need the tags that correspond to movies  in the original data (ml-1M). Luckily, since both datasets are from movielens, the movie ids (`mid`) are the same across these two datasets (i.e., not need for messy string match).\n",
    "\n",
    "(Note: Pandas' functionalities allow you to do operations similar to what you would do in SQL for relational databases.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_scores = tags_scores.loc[tags_scores['mid'].isin(ratings_pd['mid'].unique())]\n",
    "print(tags_scores.mid.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lost a few movies compared to the original count of 3706 but we can live with that.\n",
    "\n",
    "Next, instead of using the tag scores, we only keep the highly relevant tags for each movie. In other words, we assume that the presence of a tag is more meaningful than its absence. This also has the side benefit of reducing the number of available tags per movie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keep only high-relevance tags (here this is defined as having a relevance above 0.9)\n",
    "print('unique tags:', tags_scores['tid'].nunique())\n",
    "tags_scores_high = tags_scores.loc[tags_scores['relevance'] > 0.9]\n",
    "print('unique tags w. high relevance:', tags_scores_high['tid'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some understanding of how these tags are used. To help, we first build a `dataFrame` that contains, the tag names, the movie name, and its relevance (recall that currently these are in three different tables, as the tag relevance contains tag and movie IDs but not their names). \n",
    "\n",
    "Pandas' `merge` function can be used to join two dataFrames that share a common key. (This is an operation inspired by inner joins in SQL.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tags_high_names_movies = pd.merge(tags_scores_high, tags_names, how='inner', on='tid')\n",
    "tags_high_names_movies = pd.merge(tags_high_names_movies, movies_pd, how='inner', on='mid')\n",
    "display(tags_high_names_movies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly as above we can search for top movies according to a particular tag: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'scary' # This is the (sub) tag we search for\n",
    "display(tags_high_names_movies[\n",
    "    tags_high_names_movies.tName.str.contains(tag, \n",
    "                                               regex=False, case=False)].sort_values(by=['relevance'],\n",
    "                                                                                    ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next step will allow us to further explore the tags data while providing an additional step of pre-processing that will be helpful for fitting this data. \n",
    "\n",
    "In the current dataset, every movie-tag id pair is a separate entry (row of the `dataFrame`). Thinking ahead, we will want to use all tags as covariates to predict a single rating. As such, we want to construct a data matrix where each line corresponds to a single example. \n",
    "\n",
    "To do so, we re-encode `tid`s using a 1-of-K encoding (also known as using dummy variables). This is important to encode categorial variables (e.g., \"cats\" and \"dogs\") which may be represented numerically but but which cannit be ordered. For example, here each tag has a numerical index (e.g., tag `scary` is id `882`) but tags cannot be compared using their numbers (e.g., tag `882` is not \"bigger\" than tag `880` or smaller than tag `900`). 1-of-K encoding deals with this by encoding each tag as a binary vector of length $K$ with a single non-zero value which corresponds to the tag. In the present case, $K=968$ tags, and tag `scary` would have a `1` at position `882`. \n",
    "\n",
    "Below we see that our data now has 971 columns: 968 for tag ids, 1 for `mid`, and 1 for `relevance`, and 1 for the pandas index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tags_scores_high.shape)\n",
    "tags_scores_high_dum = pd.get_dummies(tags_scores_high, columns=['tid'])\n",
    "tags_scores_high_dum = tags_scores_high_dum.reset_index()\n",
    "#print(tags_scores_high_dum.shape)\n",
    "display(tags_scores_high_dum.head())\n",
    "tags_per_movie = tags_scores_high_dum.groupby(\"mid\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data we can then explore the distribution of movies per tag (and tags per movie below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = tags_scores_high.groupby(\"tid\").count()\n",
    "hists = th.hist(bins=50, column=\"mid\", xlabelsize=15, ylabelsize=15)[0][0]\n",
    "hists.set_ylabel(\"Tags\", size=15)\n",
    "hists.set_xlabel(\"Movies per tag\", size=15)\n",
    "hists.set_title(\"Histogram of Movies per tag\", size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this histogram each bar corresponds to the number of tags (y-axis) associated with a particular number of movies. For example, there are 350 tags that were used to tag a small number of movies (<5). On the other hand, the most popular tag was used to tag 210 movies.\n",
    "\n",
    "We note that the distribution is heavily skewed to the left which indicates that most tags are only used on a small number of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tname = tags_names.at[tags_names.tid.eq(th.mid.idxmax()).idxmax(), 'tName']\n",
    "print(f'The most popular tag \"{tname}\" has been used for {th.mid.max()} movies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same recipe, we can do something similar for movies instead of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists = tags_scores_high.groupby(\"mid\").count().hist(bins=40, column=\"tid\", xlabelsize=15, ylabelsize=15)\n",
    "hists[0][0].set_ylabel(\"Movies\", size=15)\n",
    "hists[0][0].set_xlabel(\"Tags per movie\", size=15)\n",
    "hists[0][0].set_title(\"Histogram of Tags per movie\", size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this histogram each bar corresponds to the number of movies (y-axis) associated with a particular number of tags. For example, there are a bit less than 500 movies that received exactly 1 tags. On the other hand, the most popular movie received almost 40 tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "What is the most popular movie in terms of tag (the one with almost 40 tags)? Bonus: Can you list the top 5 movies in terms of number of tags? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh = ...\n",
    "mname = ...\n",
    "print(f'The most popular movie \"{mname}\" has {mh.tid.max()} tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few steps we further pre-process our data in order to create a dataset for supervised learning. Recall, that we wish to predict user-movie preferences from user and movie features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join users, ratings, and tags\n",
    "data_pd = pd.merge(users_pd, ratings_pd, how='inner', on='uid')\n",
    "data_pd = pd.merge(data_pd, tags_per_movie, how='inner', on='mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this tutorial, we will only use a small fraction of our dataset to ensure that all operations (and especially model fitting) can be done in a matter of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data and keep 2% of the ratings.\n",
    "# (this small percentage ensures that all computations in this tutorial are fast)\n",
    "data_pd = data_pd.sample(frac=0.02, random_state=1234)\n",
    "print(data_pd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at our current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Final descriptive stats of our dataset.')\n",
    "print('\\t- %d items'   % data_pd['mid'].nunique())\n",
    "print('\\t- %d users'   % data_pd['uid'].nunique())\n",
    "print('\\t- %d ratings' % data_pd.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have several categorical variables (e.g., gender, occupation, zip, mid). Below, we transform these using dummy variables (just like we did above for tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_pd.shape)\n",
    "display(data_pd[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = ['gender','occupation','zip','mid','uid']\n",
    "data_pd_dum = pd.get_dummies(data_pd, columns=cols)\n",
    "print(data_pd_dum.shape)\n",
    "display(data_pd_dum.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we used pandas to create dummies, `scikit-learn` has similar capacities. The `preprocessing` module is detailed [here](https://scikit-learn.org/stable/modules/preprocessing.html). You can also checkout the section on [Categorical features](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to construct our first dataset. We will first use a subset of the columns (not including tags). \n",
    "\n",
    "Below you will also note that we split our data into train and test using `train_test_split` from scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = \"mid_*|uid_*|gender_*|age|zip_*|occupation_*\"\n",
    "X = data_pd_dum.filter(regex=('('+attributes+')')) \n",
    "print(X.shape)\n",
    "\n",
    "rating = data_pd_dum['rating']\n",
    "print(rating.shape)\n",
    "\n",
    "# Split Train/Test\n",
    "# Keep 20% of the data for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, rating, test_size=0.2, random_state=1234, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recommender Systems note:* We split the dataset without looking at users and items. In other words, more active users and popular items will be (on average) more represented in the heldout data. If this is not desired, for example one may wish a fairer treatment of users. We could then ensure that each user has the same amount of data in the heldout set (and similarly for items). \n",
    "\n",
    "*Recommender Systems note #2:* Practically speaking it would make more sense to divide ratings by timestamp. That is, train on ratings up to some date and test on ratings after that date (future ratings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modelling\"></a>\n",
    "# Section 2: Mod√©lisation \n",
    "\n",
    "Ratings ([likert scale](https://en.wikipedia.org/wiki/Likert_scale)) are ordinal quantities. However, for ease of modelling and evaluation we treat ratings as real-valued (we will discuss in greater length some of these issues later this semester). Concretely, we will measure the loss/error using a mean-squared error function:\n",
    "\n",
    "$$ \\text{MSE}(f(x),y) := \\frac{1}{n} \\sum_{i=0}^n (f(x_i) - y_i)^2$$ \n",
    "\n",
    "The MSE can be understood as the average square distance between the predictor $f(x_i)$ and the target $y_i$. MSE returns a non-negative quantity and the perfect predictor has an MSE of $0$. If Model 1 has a smaller MSE than Model 2, its performance is higher according to that metric.\n",
    "\n",
    "*Train/Test:* Recall that while we estimate the parameters of the model using the **train** data, we evaluate the quality of the model (its performance) using **test** data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 A first model: mean predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often helpful to use a very simple benchmark to compare against the performance of our models.\n",
    "\n",
    "Our initial benchmark is a model which simply predicts the mean (train) rating.\n",
    "\n",
    "*Recommender Systems note:* We could obtain a slightly better model by predicting with a user- or item-specific mean instead of the global mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy of constant predictor\n",
    "\n",
    "print(\"Constant predictor\")\n",
    "\n",
    "print(\"\\tTrain mean-squared error: %.3f\"\n",
    "      % mean_squared_error(y_train, \n",
    "                           np.full_like(y_train, y_train.mean())))\n",
    "print(\"\\tTest mean-squared error: %.3f\"\n",
    "      % mean_squared_error(y_test, \n",
    "                           np.full_like(y_test, y_train.mean())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test errors are just about the same (I imagine that difference is not statistically significant). Our model is very simple (in machine learning term it has a high bias) and so you would not expect its performance to fluctuate much on the test set (i.e., small variance). \n",
    "\n",
    "In terms of absolute values these indicate that, on average, our predictions are 1.3 units ($\\sqrt{1.6}$) away from the true rating. This indicates that you shouldn't be too surprised that the model gives a rating below 4 to a movie that you would rate as a 5. Having said that, it is difficult to know how good this is before we compare to other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Linear regression\n",
    "\n",
    "For our second model, we will fit a linear regression that uses user features to predict ratings. In particular we use the users' gender, age, zip, and occupation. We fit this model\n",
    "$$\n",
    "f(x_{ui}) = \\theta_\\text{gender} x_{\\text{gender}_u} + \\theta_{\\text{age}} x_{\\text{age}_u} + \\theta_\\text{zip} x_{\\text{zip}_u} + \\theta_\\text{occupation} x_{\\text{occupation}_u} + \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i}\n",
    "$$ \n",
    "\n",
    "$\\theta_{1:6}$ are the parameters, $\\text{gender}_u$ stands for the gender of user $u$ and similarly for other covariates. Also, $x_{\\text{uid}_u}$ represents the identity of the user and similarly for $x_{\\text{mid}_i}$ and movies.\n",
    "\n",
    "Note that some of these variables are categorial so in fact they are associated with a vector of parameters. For example, zip is a categorical variable with 100 different possible values and so $\\theta_{\\text{zip}}$ has 100 dimensions. \n",
    "\n",
    "Training this model involves minimizing the train MSE, this is exactly what the `LinearRegression` class does. (This is a [least-squares problem](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html) and it can be solved in closed form.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "print(\"Number of parameters: \", reg.coef_.shape[0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train predictions\n",
    "y_train_pred = reg.predict(X_train)\n",
    "\n",
    "print(\"Train Mean squared error: %.3f\"\n",
    "      % mean_squared_error(y_train, y_train_pred))\n",
    "\n",
    "# Make test predictions\n",
    "y_test_pred = reg.predict(X_test)\n",
    "\n",
    "print(\"Test Mean squared error: %.3f\"\n",
    "      % mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that train error $<<$ test error ($<<$ stands for \"much smaller\"). This is a clear case of overfitting. That is, the model has learned the training data and cannot generalize to new unseen data (it's a low bias and high variance model). \n",
    "\n",
    "Different methods can help prevent the model from overfitting, this is often referred to as *regularizing* the model. Here we will add a penalty that constrains the learned parameters to stay close to zero. Intuitively, this learns a function that is smoother and, the hope is, that generalizes better. This penalty or regularizer is added to the loss function which becomes: $$  \\text{loss} := \\text{MSE} + \\alpha \\sum_i ||\\theta_i||_2^2 $$\n",
    "\n",
    "Instead of the previous $ \\text{loss} := \\text{MSE} $. \n",
    "\n",
    "Note: \n",
    "- $||\\cdot||_2$ stands for the 2-Norm. That is, the square root of the sum of the operand's squared elements.\n",
    "- $\\alpha$ is a hyper-parameter which denotes the strength of the regularizer (if $\\alpha=0$ the regularizer vanishes and if $\\alpha=\\infty$ all parameters must be equal to exactly 0). A hyperparameter is a parameter that is not learned during training but set a priori (here, learning $\\alpha$ along with the $\\theta$s would lead to a $\\alpha=0$).\n",
    "\n",
    "During learning the model must then tradeoff performance (MSE) and complexity (high $\\theta$s). There are different names for this particular regularizer including weight decay, L2-regularization, and ridge (regression). Scikit-learn offers the `Ridge` class from the `linear_model` package to fit regularized linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.Ridge(alpha=10)\n",
    "\n",
    "# Train the model using the training sets\n",
    "start = time.time()\n",
    "regr.fit(X_train, y_train)\n",
    "fit_time = time.time() - start\n",
    "\n",
    "print(\"Fitting time: %.2f seconds\" % fit_time)\n",
    "\n",
    "print(\"Number of parameters:\", regr.coef_.shape[0]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3** Explain why there are 7,184 parameters.\n",
    "\n",
    "**Hint:** Don't forget the intercept/bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train predictions\n",
    "y_train_pred = regr.predict(X_train)\n",
    "\n",
    "print(\"Train Mean squared error: %.3f\"\n",
    "      % mean_squared_error(y_train, y_train_pred))\n",
    "\n",
    "# Make test predictions\n",
    "y_test_pred = regr.predict(X_test)\n",
    "\n",
    "print(\"Test Mean squared error: %.3f\"\n",
    "      % mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to above, we see that with $\\alpha=10$ the train and test errors are much closer (i.e., there's less overfitting). Presumably different values of $\\alpha$ would yield different generalizations.\n",
    "\n",
    "\n",
    "**Question 4** How do you find the \"best\" value of $\\alpha$ for a given model and dataset?\n",
    "\n",
    "**Hint:** Have a look at the [RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV), a cross-validation enabled version of Ridge.\n",
    "\n",
    "**Answer:** See below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regRCV = ... \n",
    "\n",
    "# Train the model using the training sets\n",
    "\n",
    "\n",
    "print(\"Number of parameters: %d, estimated alpha: %d\" % (regRCV.coef_.shape[0], regRCV.alpha_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technical remark: since the optimization is often done in log space, it's typical for the set of $\\alpha$'s to be powers of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train predictions\n",
    "y_train_pred = ...\n",
    "\n",
    "print(\"Train Mean squared error: %.3f\"\n",
    "      % mean_squared_error(y_train, y_train_pred))\n",
    "\n",
    "# Make test predictions\n",
    "y_test_pred = ... \n",
    "\n",
    "print(\"Test Mean squared error: %.3f\"\n",
    "      % mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of doing cross validation (for example using `RidgeCV`) is clear. It automatically searches for the best values of hyperparameters (here $\\alpha$) from a given set (here  $\\{ 1, 10, 100 \\}$). \n",
    "\n",
    "Cross validation should always be used to search for good hyperparameters (especially for non-linear models different values of hyperparameters may give very different results). In certain cases you may have to manually implement cross validation. In such cases you will likely need to split a separate validation set from your training data--in `sklearn` you can use the `train_test_split` function. It is typical for the validation set to be the same size as the test set. \n",
    "\n",
    "You should also remember to **never select model hyperparameters based on performance on test set**, this would give you over-optimistic results because you are effectively using your test set to tune your models (its hyperparameters). The main purpose of the test set is to provide an unbiased way of comparing different models. \n",
    "\n",
    "***\n",
    "\n",
    "Armed with a good model we can explore the learned model including some of its predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to return non-zero columns\n",
    "def non_zero(row, columns):\n",
    "    col_name = list(columns[~(row == 0)])[0]\n",
    "    #r = re.sub('mid_','',l)\n",
    "    return col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of ratings per movie (popularity)\n",
    "mids = X_test.filter(regex=('mid_*'))\n",
    "y_mid_cols = mids.apply(lambda x: non_zero(x, mids.columns), axis=1)\n",
    "movie_popularity = X_train.filter(regex=('mid_*')).sum(axis=0)[ y_mid_cols ]\n",
    "\n",
    "# get number of ratings per user (activity)\n",
    "uids = X_test.filter(regex=('uid_*'))\n",
    "y_uid_cols = uids.apply(lambda x: non_zero(x, uids.columns), axis=1)\n",
    "user_activity = X_train.filter(regex=('uid_*')).sum(axis=0)[ y_uid_cols ]\n",
    "\n",
    "err = (y_test_pred-y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only plot a subsample for higher readability\n",
    "subn = 500 \n",
    "fig, (ax0, ax1) = plt.subplots(ncols=2)\n",
    "fig.set_figwidth(15)\n",
    "ax0.scatter(movie_popularity[:subn], err[:subn])\n",
    "ax0.set_ylabel('Prediction error')\n",
    "ax0.set_xlabel('Movie Popularity')\n",
    "\n",
    "ax1.scatter(user_activity[:subn], err[:subn])\n",
    "ax1.set_ylabel('Prediction error')\n",
    "ax1.set_xlabel('User Activity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we plotted the prediction error of (a subset of) test ratings compared to the popularity of movies (left) and activity level of users (right). We note that:\n",
    "\n",
    "- This empirical distribution looks symmetrical so there doesn't seem to be a bias toward lower or higher predictions\n",
    "- The prediction errors seem to show that movies and users with more data have smaller prediction error (i.e., the data forms a \"triangle\" pointing to the right, this is much clearer when running w. more training data which we limit here to save time). This is intuitive, the more data you have about an item the more accurate should be the estimation of its parameters ($\\theta_{\\text{mid}}$). This could also be reinforced by the fact that we are splitting ratings randomly for train and test (versus splitting by user or item). Hence, popular movies and high-activity users have a great influence in the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Linear regression with tag features/covariates\n",
    "\n",
    "We use a linear regression model as above but also model movie tags: \n",
    "\n",
    "$$\n",
    "f(x_{ui}) = \\theta_\\text{gender} x_{\\text{gender}_u} + \\theta_{\\text{age}} x_{\\text{age}_u} + \\theta_\\text{zip} x_{\\text{zip}_u} + \\theta_\\text{occupation} x_{\\text{occupation}_u} + \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i} \\mathbf{+ x_{\\text{tags}_i}\\boldsymbol\\theta_\\text{tags}}\n",
    "$$ \n",
    "\n",
    "The last term on the right-hand side (bolded) is the only difference wrt to our previous model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** How do you think that this new model will compare to the previous model of Section 2.2? Can you say something definitive about its performance? \n",
    "\n",
    "**Hint:** One model is a special case of another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is very similar to how we constructed our dataset above except that we add the tags columns\n",
    "X_tags = data_pd_dum.filter(regex=('('+attributes+\"|tid_*\"+')'))\n",
    "print(X_tags.shape)\n",
    "\n",
    "# Split Train/Test. Notice that we use the same seed as above to replicate that split.\n",
    "X_train_tags, X_test_tags, y_train_tags, y_test_tags = train_test_split(\n",
    "    X_tags, rating, test_size=0.2, random_state=1234, shuffle=False)\n",
    "print(X_train_tags.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr_tags = linear_model.Ridge(alpha=10)\n",
    "\n",
    "# Train the model using the training sets\n",
    "start = time.time()\n",
    "regr_tags.fit(X_train_tags, y_train_tags)\n",
    "fit_time = time.time() - start\n",
    "\n",
    "print(\"fitting time: %.2f seconds\" % fit_time)\n",
    "print(\"number of parameters:\", regr_tags.coef_.shape[0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train predictions\n",
    "y_train_pred = regr_tags.predict(X_train_tags)\n",
    "\n",
    "print(\"Train Mean squared error: %.4f\"\n",
    "      % mean_squared_error(y_train_tags, y_train_pred))\n",
    "\n",
    "# Make test predictions\n",
    "y_test_pred = regr_tags.predict(X_test_tags)\n",
    "\n",
    "print(\"Test Mean squared error: %.4f\"\n",
    "      % mean_squared_error(y_test_tags, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks: We obtain a test MSE of 0.99 for model 2.3 compared to a test MSE of 1.03 for model 2.2. So the tags do seem to provide slightly better test performance at the expense of a slightly larger model (968 extra parameters to fit) which takes about 30% longer to fit. Take this with a grain of salt because it is hardware dependent. But, this hints at the fact that the fitting algorithm is not linear (i.e., increasing the number of parameters by 10% yields an increase of 30% in running time).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Fitting a non-linear model \n",
    "\n",
    "So far we have studied the effect of using different features on a linear model. We note that adding features seems to help, as long as the features provide novel and predictive information and are not redundant. \n",
    "\n",
    "Here we explore a non-linear model, a one-hidden layer neural network for regression. The task and the data are the same as before, only the model changes.\n",
    "\n",
    "**Neural network basics:** (We will discuss these models in some depth over the next two weeks.)\n",
    "- A neural network is made up of interconnected *neurons*. Each neuron computes a few simple operations. \n",
    "- In a *feed-forward network*, neurons are organized into sets called *layers*. \n",
    " - Neurons in each layer get their inputs from the previous layer and send their outputs to the next layer. \n",
    " - The first layer is called the *input layer* it provides data to the next layer. The last layer is the *output layer* it provides a prediction $\\hat{y}$. \n",
    " - Layers in between the input layer and the output layer are called *hidden layers*. Each neuron in the hidden layers is a linear regression model followed by a non-linear function (*activation function*): $f(x) = \\sum_i x_i \\theta_i$. \n",
    "  - The number of neurons in the input and output layers are fixed by the data (the number of features and the number of predictions). \n",
    "  - The number of neurons of a hidden layer is a hyperparameter. Another hyperparameter is the number of hidden layers. \n",
    "\n",
    "Mathematically for a regression task (with a single output), a one-hidden layer neural net is: \n",
    "$$ \n",
    "f(x) = f_\\text{o} ( \\sum_{j=0}^{|\\text{hidden n.}|} \\theta'_{j} f_\\text{h}( \\sum_{i=0}^{|p|}\\theta_{ij} x_i ) ) \n",
    "$$ \n",
    "where\n",
    "- $\\theta_{ij}$ are the parameters of input $i$ and neuron $j$ in the hidden layer.\n",
    "- $f_h$ is the activation function of the hidden layer\n",
    "- $\\theta'_{j}$ are the parameters that connect the neuron $j$ in the hidden layer to the output layer.\n",
    "- $f_o$ is the activation function of the output layer\n",
    "\n",
    "An intuitive way of visualization a neural net (especially large ones) is to draw neurons as nodes and connections between neurons as arcs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 1 hidden layer neural net, where the input has 10 dimensions (p=10) and the output 1\n",
    "input_dims = 10 # p \n",
    "hidden_layers_size = [4] # number of hidden neurons for each hidden layer (adding a dimension adds a layer)\n",
    "output_dims = 1 # number of outputs\n",
    "\n",
    "network = DrawNN( [input_dims] + hidden_layers_size + [output_dims] )\n",
    "network.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit a neural network on this data. \n",
    "regr_nn = neural_network.MLPRegressor(alpha=0.1, # l2-regularization (weight decay)\n",
    "                                      hidden_layer_sizes=tuple(hidden_layers_size),\n",
    "                                      early_stopping=True, # stop if validation performance decreases\n",
    "                                      verbose=True,\n",
    "                                      random_state=1234)\n",
    "start = time.time()\n",
    "regr_nn.fit(X_train_tags, y_train_tags)\n",
    "fit_time = time.time() - start\n",
    "\n",
    "print(\"fitting time: %.2f seconds\" % fit_time)\n",
    "print(\"number of parameters:\", reduce(lambda x,y: x+y, \n",
    "                                       list(map(lambda x: x.size, regr_nn.coefs_+regr_nn.intercepts_)) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like previous models we can regularize a neural net to combat overfitting:\n",
    "- Here we use the same L2-penalty regularizer on all parameters. The strength of this regularizer is given by $\\alpha$. \n",
    "- In addition, we use a second regularizer called `early-stopping`. Learning the parameters of a neural network is done iteratively using a method called gradient descent (as opposed to linear regression, there is no analytical solution for the parameters given the objective function). Early stopping simply evaluates the validation error after each iteration. It stops learning when the validation error stops improving. This can happen before the training loss converges. When it does not, then this regularizer has no effect on learning. In `scikit-learn`, the MLPRegressor class with `early_stopping=True` automatically splits a validation set from the training set to be used by this regularizer. The disadvantage, of course, is that this reduces the amount of data used to fit parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** Why does this model come with the possibility to set the random seed (i.e., `random_state`) while linear regression did not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train predictions\n",
    "y_train_pred = regr_nn.predict(X_train_tags)\n",
    "\n",
    "print(\"Train Mean squared error: %.4f\"\n",
    "      % mean_squared_error(y_train_tags, y_train_pred))\n",
    "\n",
    "# Make test predictions\n",
    "y_test_pred = regr_nn.predict(X_test_tags)\n",
    "\n",
    "print(\"Test Mean squared error: %.4f\"\n",
    "      % mean_squared_error(y_test_tags, y_test_pred))\n",
    "#Train Mean squared error: 0.6623\n",
    "#Test Mean squared error: 1.0465"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our updated table of results\n",
    "\n",
    "| Model        | Test MSE           |\n",
    "| ------------- |:-------------:| \n",
    "| 2.2 (Linear Reg. w. basic features)     | 1.031 | 0.865  |\n",
    "| 2.3 (2.2 + movie tags)     | 0.991 | 0.857 |\n",
    "| 2.4 (Neural Network w. features from 2.3) | 1.029 | 0.865 | \n",
    "\n",
    "Although neural networks are very powerful models, on this task the performance of our neural net does not outperform a simpler linear regression model. This of course does not mean that a different neural net (for example, one with more neurons per layer or more layers or just one trained from different hyperparameters) could not do better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"concluding-remarks\"></a>\n",
    "### Section 3. Concluding Remarks\n",
    "\n",
    "The goal of this tutorial was to put in practice some of the principles that we have discussed since the start of this course (and hint at some of the things that are coming in the next weeks). We will spend one full week (week #11) thinking about preference data including where it's coming from and how to model it.\n",
    "\n",
    "Here are a few more parting thoughts: \n",
    "\n",
    "#### Machine Learning\n",
    "\n",
    "As you might have noticed, applied machine learning is a very empirical endeavour. Once you have data in the right format, it is typical to fit it using several models, each time trying to understand the advantages/disadvantages of each model and getting a more thorough understanding of the data. In practice, this last part may be crucial and we did not adress it much in this tutorial (instead focussing on the models themeselves).\n",
    "\n",
    "\n",
    "#### Scikit-learn\n",
    "`scikit-learn` is a powerful ML library. It is meant as a model (and data pre-processing) toolbox. It provides an interface to a wide variety of models, it is actively developped, and in general seen as a very good plateform. It is also open source and free to use.\n",
    "\n",
    "Model Selection, i.e., which model should I use for a particular dataset/task can be daunting. [This page](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) provides some tips particular to `scikit-learn`. In general, when working on a new task/dataset it is often useful to try and compare different models. Remember that in practice (mean) test-performance is only one of the possible desiderata (others include running time both for training and other metrics like false positive rates).\n",
    "\n",
    "Note that `scikit-learn` does not fit every use case. For example, its support of modern neural networks is fairly modest. It is also not meant as a development plateform for new models.\n",
    "\n",
    "#### Other packages\n",
    "\n",
    "Software is one of the reasons behind the rise of modern machine learning. Modern software automatize a number of tasks which allow programmers and researchers alike to be much more efficient. `scikit-learn` is one popular package for fitting machine learning models but there exist others (both applied and also for exploration/development purposes). All in all, it can be useful to know about these packages but (in my opinion) it is more useful to have a good understanding of the fundamentals of the field as the package landscape changes rapidly and one can always learn one more. \n",
    "\n",
    "\n",
    "#### Food for thought\n",
    " - In our models we assumed that ratings were the dependent variable (y) and that we had covariates (e.g., features of users and movies). Imagine a setting where we don't have any features or, somewhat equivalently, only have features that end up not being predictive of ratings. In that case the linear regression model would be: \n",
    " \n",
    "$$\n",
    "f(x_{ui}) = \\theta_\\text{uid} x_{\\text{uid}_u} + \\theta_{\\text{mid}} x_{\\text{mid}_i}\n",
    "$$ \n",
    " \n",
    " \n",
    "- **Question 7:** What's wrong with the above model? Try to think about it for a minute or two before looking at the answer.\n",
    "\n",
    "\n",
    "\n",
    "- As we will see during week 11 (on recommender systems), many models take ratings as output **and** as input. For example, one could take a user's previous ratings and try to predict one's future ratings (for example using an auto-encoder model). This is a nice way to build models that do not require any user/item covariates (and these models can also be extended when that data exist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\n",
    "Scikit-learn\n",
    "- [Documentation](https://scikit-learn.org/stable/documentation.html)\n",
    "- [Tutorials](https://scikit-learn.org/stable/tutorial/index.html)\n",
    "- [Help for model selection](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
